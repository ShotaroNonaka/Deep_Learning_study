{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d064c2d-d25f-497f-99df-845e87ab891c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-14 08:49:11,205 [nnabla][INFO]: Initializing CPU extension...\n"
     ]
    }
   ],
   "source": [
    "import yolov2\n",
    "from draw_utils import DrawBoundingBoxes\n",
    "\n",
    "import nnabla as nn\n",
    "import nnabla.functions as F\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from nnabla.utils.image_utils import imread, imresize, imsave\n",
    "\n",
    "import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8feea426-b41a-4163-8969-7cefb220c2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 608\n",
    "weights = 'yolov2.h5'\n",
    "classes = 80\n",
    "class_names = 'coco.names'\n",
    "thresh = .22\n",
    "nms = .45\n",
    "nms_per_class = True\n",
    "num_anchors = 5\n",
    "anchors = [0.57273, 0.677385, 1.87446, 2.06253, 3.33843, 5.47434, 7.88282, 3.52778, 9.77052, 9.16828]\n",
    "anchors = np.array(anchors).reshape(-1, 2)\n",
    "\n",
    "args_input = 'dog.jpg'\n",
    "\n",
    "\n",
    "context = 'cudnn'\n",
    "dev_id = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f75baafe-3273-49ab-a70e-e3570bd746db",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'a person looking down'\n",
    "\n",
    "\n",
    "with nn.parameter_scope('clip'):\n",
    "    clip.load('data/ViT-cpu.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1659531c-b068-4eab-85ac-ebc8dc78814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize(img, mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711), max_pixel_value=255.0):\n",
    "    mean = np.array(mean, dtype=np.float32)\n",
    "    mean = nn.Variable.from_numpy_array(mean)\n",
    "    mean *= max_pixel_value\n",
    "\n",
    "    std = np.array(std, dtype=np.float32)\n",
    "    std = nn.Variable.from_numpy_array(std)\n",
    "    std *= max_pixel_value\n",
    "\n",
    "    denominator = F.r_div_scalar(std)\n",
    "\n",
    "    mean = F.reshape(mean, (3, 1, 1))\n",
    "    denominator = F.reshape(denominator, (3, 1, 1))\n",
    "    \n",
    "    img -= mean\n",
    "    img *= denominator\n",
    "    return img\n",
    "\n",
    "def clip_logits(image, text):\n",
    "    with nn.parameter_scope('clip'):\n",
    "        with nn.auto_forward():\n",
    "            h, w, c = image.shape\n",
    "\n",
    "            # big box\n",
    "            box = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "            \n",
    "            if h > w:\n",
    "                res_w = 224 * w // h\n",
    "                image = imresize(image, (res_w, 224), interpolate='bicubic')\n",
    "                s = int(round(224 - res_w) / 2.)\n",
    "                box[:, s:s+res_w, :] = image\n",
    "\n",
    "            else:\n",
    "                res_h = 224 * h // w\n",
    "                image = imresize(image, (224, res_h), interpolate='bicubic')\n",
    "                s = int(round(224 - res_h) / 2.)\n",
    "                box[s:s+res_h, :, :] = image\n",
    "            \n",
    "            image = box\n",
    "            # ceter crop\n",
    "#             if h < w:\n",
    "#                 res_w = 224 * w // h\n",
    "#                 crop_left = int(round((res_w - 224) / 2.))\n",
    "#                 image = imresize(image, (res_w, 224), interpolate='bicubic')\n",
    "#                 image = image[:, crop_left:crop_left+224, :]\n",
    "\n",
    "#             else:\n",
    "#                 res_h = 224 * h // w\n",
    "#                 crop_top = int(round((res_h - 224) / 2.))\n",
    "#                 image = imresize(image, (224, res_h), interpolate='bicubic')\n",
    "#                 image = image[crop_top:crop_top+224, :, :]\n",
    "\n",
    "            \n",
    "            image = image.transpose(2, 1, 0)\n",
    "            \n",
    "            \n",
    "            image = nn.Variable.from_numpy_array(image)\n",
    "            \n",
    "    \n",
    "            image = _normalize(image)\n",
    "\n",
    "            text = clip.tokenize(text)\n",
    "\n",
    "            img_logits, _ = clip.logits(image, text)\n",
    "            similarity = img_logits / 100\n",
    "    \n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a387b26-ab38-4de0-9a22-a342cf4824e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-14 08:49:13,069 [nnabla][INFO]: Initializing CUDA extension...\n",
      "2021-09-14 08:49:13,082 [nnabla][INFO]: Initializing cuDNN extension...\n"
     ]
    }
   ],
   "source": [
    "names = np.genfromtxt(class_names, dtype=str, delimiter='?')\n",
    "rng = np.random.RandomState(1223)\n",
    "colors = rng.randint(0, 256, (classes, 3)).astype(np.uint8)\n",
    "colors = [tuple(c.tolist()) for c in colors]\n",
    "\n",
    "\n",
    "# Set context\n",
    "from nnabla.ext_utils import get_extension_context\n",
    "ctx = get_extension_context(\n",
    "    context, device_id=dev_id, type_config='float')\n",
    "nn.set_default_context(ctx)\n",
    "\n",
    "_ = nn.load_parameters(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b60c6a8e-a823-4972-a350-93b27df7085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bounding_boxes(img, bboxes, im_w, im_h, names, colors, sub_w, sub_h, thresh, text):\n",
    "    draw = DrawBoundingBoxes(img, colors)\n",
    "    for bb in bboxes:\n",
    "        if bb[4] <= 0:\n",
    "            continue\n",
    "        # x, y, w, h = bb[:4] * np.array([im_w, im_h, im_w, im_h])\n",
    "        x, y, w, h = bb[:4]\n",
    "        x = (x - (1 - sub_w) / 2.) / sub_w * im_w\n",
    "        y = (y - (1 - sub_h) / 2.) / sub_h * im_h\n",
    "        w = w * im_w / sub_w\n",
    "        h = h * im_h / sub_h\n",
    "        dw = w / 2.\n",
    "        dh = h / 2.\n",
    "        x0 = int(np.clip(x - dw, 0, im_w))\n",
    "        y0 = int(np.clip(y - dh, 0, im_h))\n",
    "        x1 = int(np.clip(x + dw, 0, im_w))\n",
    "        y1 = int(np.clip(y + dh, 0, im_h))\n",
    "        \n",
    "\n",
    "        # prob check\n",
    "        det_ind = np.where(bb[5:] > 0.1)[0]\n",
    "        if len(det_ind) == 0:\n",
    "            continue\n",
    "        \n",
    "#         print('PASS 1')\n",
    "        \n",
    "        cand = img[y0:y1, x0:x1, :]\n",
    "        \n",
    "        print(x0, x1, y0, y1)\n",
    "\n",
    "        prob = clip_logits(cand, text)\n",
    "        \n",
    "        print(f'prob: {prob.d[0, 0]}')\n",
    "        \n",
    "#         if prob.d[0, 0] <= thresh:\n",
    "#             continue\n",
    "            \n",
    "#         print('PASS 2')\n",
    "            \n",
    "        label = ''\n",
    "        draw.draw((x0, y0, x1, y1), 0, label)\n",
    "        \n",
    "#         # prob check\n",
    "#         det_ind = np.where(bb[5:] > thresh)[0]\n",
    "#         if len(det_ind) == 0:\n",
    "#             continue\n",
    "#         prob = bb[5 + det_ind]\n",
    "        # Object detection with deep learning and OpenCV\n",
    "        # https://goo.gl/q4RdcZ\n",
    "#         label = ', '.join(\"{}: {:.2f}%\".format(\n",
    "#             names[det_ind[j]], prob[j] * 100) for j in range(len(det_ind)))\n",
    "#         print(\"[INFO] {}\".format(label))\n",
    "#         draw.draw((x0, y0, x1, y1), det_ind[0], label)\n",
    "    return draw.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e173132b-4fa8-41d9-beb9-dc3cecbf66ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a YOLO v2 network\n",
    "feature_dict = {}\n",
    "x = nn.Variable((1, 3, width, width))\n",
    "y = yolov2.yolov2(x, num_anchors, classes,\n",
    "                    test=True, feature_dict=feature_dict)\n",
    "y = yolov2.yolov2_activate(y, num_anchors, anchors)\n",
    "y = F.nms_detection2d(y, thresh, nms, nms_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "682f06d9-848c-4a3c-bb9d-726b59b8b20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Read image\n",
    "img_orig = imread(args_input, num_channels=3)\n",
    "im_h, im_w, _ = img_orig.shape\n",
    "# letterbox\n",
    "w = width\n",
    "h = width\n",
    "\n",
    "\n",
    "\n",
    "if (w * 1.0 / im_w) < (h * 1. / im_h):\n",
    "    new_w = w\n",
    "    new_h = int((im_h * w) / im_w)\n",
    "else:\n",
    "    new_h = h\n",
    "    new_w = int((im_w * h) / im_h)\n",
    "\n",
    "patch = imresize(img_orig, (new_w, new_h)) / 255.\n",
    "img = np.ones((h, w, 3), np.float32) * 0.5\n",
    "# resize\n",
    "x0 = int((w - new_w) / 2)\n",
    "y0 = int((h - new_h) / 2)\n",
    "img[y0:y0 + new_h, x0:x0 + new_w] = patch\n",
    "\n",
    "# Execute YOLO v2\n",
    "print(\"forward\")\n",
    "in_img = img.transpose(2, 0, 1).reshape(1, 3, width, width)\n",
    "x.d = in_img\n",
    "y.forward(clear_buffer=True)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f61c380b-1b9a-43c8-8d51-e92e795d25ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes = y.d[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ac8afd7-1aa4-4504-8636-8206db9b3d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1805, 85)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bboxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61062e6e-2fa0-4592-82ce-17704bf1bbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132 320 232 520\n",
      "prob: 0.21061177551746368\n",
      "466 680 84 168\n",
      "prob: 0.2581664025783539\n",
      "94 589 122 449\n",
      "prob: 0.21361331641674042\n"
     ]
    }
   ],
   "source": [
    "img_draw = draw_bounding_boxes(\n",
    "    img_orig, bboxes, im_w, im_h, names, colors, new_w * 1.0 / w, new_h * 1.0 / h, thresh, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea6dca40-7f41-4ffc-8392-baddd6696bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(576, 768, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_draw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc458ee7-dbc0-4870-b154-776912ac72e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1805, 85)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bboxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84e03f06-42e8-45d5-9319-46f1942e9276",
   "metadata": {},
   "outputs": [],
   "source": [
    "imsave('tada.jpg', img_draw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9e6d413-6757-4d12-9f6c-f3a9fd453188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(576, 768, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ed8d7bc-7953-4548-9fc9-d5bda4dd53da",
   "metadata": {},
   "outputs": [],
   "source": [
    "imsave('fff.jpg', img_orig[122:449, 94:589, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90e05e0d-bb59-47c9-98e2-e2ec99a6ae45",
   "metadata": {},
   "outputs": [],
   "source": [
    "bike = img_orig[122:449, 94:589, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19916837-6c47-4341-929b-a65ae2adde74",
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w, c = bike.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00ee4c71-e399-4409-b460-1ba9c24b9a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "box = np.zeros((224, 224, 3), dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cdda99e5-27f3-431c-8d79-839e37b1f015",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_h = 224 * h // w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea0ba526-7002-4997-b2b7-e6e2b06a7ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bike = imresize(bike, (224, res_h), interpolate='bicubic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c2c0028-9baf-4f53-947f-4ec658f0ec46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(147, 224, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bike.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67e7be00-73ea-4c01-b270-6bf007ea288e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = int(round(224 - res_h) / 2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "368b02fd-45e6-4f01-8f39-ad37957f5097",
   "metadata": {},
   "outputs": [],
   "source": [
    "box[s:s+res_h, :, :] = bike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2e142fd-7190-41b0-a212-42ecc3c18de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "imsave('fff.jpg', box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f911314-0878-4d4e-82cf-415a45fba591",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
